{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#REDIRECT [[Python]]\n"
     ]
    }
   ],
   "source": [
    "# review how library works: \n",
    "\n",
    "import mwclient\n",
    "\n",
    "# Connect to Spanish Wikipedia\n",
    "site = mwclient.Site('es.wikipedia.org')\n",
    "\n",
    "# Fetch a page\n",
    "page = site.pages['Python (lenguaje de programación)']\n",
    "\n",
    "# Retrieve the page content\n",
    "content = page.text()\n",
    "\n",
    "# Print the page content\n",
    "print(content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! Falta completarlo \n",
    "# Creating datasets from wikipedia\n",
    "\n",
    "# imports\n",
    "import mwclient             # for downloading example Wikipedia articles\n",
    "import mwparserfromhell     # for splitting Wikipedia articles into sections\n",
    "import openai               # for generating embeddings\n",
    "import pandas as pd         # for DataFrames to store article sections and embeddings\n",
    "import re                   # for cutting <ref> links out of Wikipedia articles\n",
    "import tiktoken             # for counting tokens\n",
    "\n",
    "\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 article titles in Category:2023_regional_elections_in_Spain.\n"
     ]
    }
   ],
   "source": [
    "# get Wikipedia pages about the 2022 Winter Olympics\n",
    "\n",
    "CATEGORY_TITLE = \"Category:2023_regional_elections_in_Spain\"\n",
    "WIKI_SITE = \"en.wikipedia.org\"\n",
    "\n",
    "\n",
    "def titles_from_category(\n",
    "    category: mwclient.listing.Category, max_depth: int\n",
    ") -> set[str]:\n",
    "    \"\"\"Return a set of page titles in a given Wiki category and its subcategories.\"\"\"\n",
    "    titles = set()\n",
    "    for cm in category.members():\n",
    "        if type(cm) == mwclient.page.Page:\n",
    "            # ^type() used instead of isinstance() to catch match w/ no inheritance\n",
    "            titles.add(cm.name)\n",
    "        elif isinstance(cm, mwclient.listing.Category) and max_depth > 0:\n",
    "            deeper_titles = titles_from_category(cm, max_depth=max_depth - 1)\n",
    "            titles.update(deeper_titles)\n",
    "    return titles\n",
    "\n",
    "\n",
    "site = mwclient.Site(WIKI_SITE)\n",
    "category_page = site.pages[CATEGORY_TITLE]\n",
    "titles = titles_from_category(category_page, max_depth=1)\n",
    "# ^note: max_depth=1 means we go one level deep in the category tree\n",
    "print(f\"Found {len(titles)} article titles in {CATEGORY_TITLE}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk documents\n",
    "\n",
    "Now that we have our reference documents, we need to prepare them for search.Because GPT can only read a limited amount of text at once, we'll split each document into chunks short enough to be read.\n",
    "\n",
    "For this specific example on Wikipedia articles, we'll:\n",
    "\n",
    "- Discard less relevant-looking sections like External Links and Footnotes\n",
    "- Clean up the text by removing reference tags (e.g., ), whitespace, and super short sections\n",
    "- Split each article into sections\n",
    "- Prepend titles and subtitles to each section's text, to help GPT understand the context\n",
    "- If a section is long (say, > 1,600 tokens), we'll recursively split it into smaller sections, trying to - split along semantic boundaries like paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions to split Wikipedia pages into sections\n",
    "\n",
    "SECTIONS_TO_IGNORE = [\n",
    "    \"See also\",\n",
    "    \"References\",\n",
    "    \"External links\",\n",
    "    \"Further reading\",\n",
    "    \"Footnotes\",\n",
    "    \"Bibliography\",\n",
    "    \"Sources\",\n",
    "    \"Citations\",\n",
    "    \"Literature\",\n",
    "    \"Footnotes\",\n",
    "    \"Notes and references\",\n",
    "    \"Photo gallery\",\n",
    "    \"Works cited\",\n",
    "    \"Photos\",\n",
    "    \"Gallery\",\n",
    "    \"Notes\",\n",
    "    \"References and sources\",\n",
    "    \"References and notes\",\n",
    "]\n",
    "\n",
    "\n",
    "def all_subsections_from_section(\n",
    "    section: mwparserfromhell.wikicode.Wikicode,\n",
    "    parent_titles: list[str],\n",
    "    sections_to_ignore: set[str],\n",
    ") -> list[tuple[list[str], str]]:\n",
    "    \"\"\"\n",
    "    From a Wikipedia section, return a flattened list of all nested subsections.\n",
    "    Each subsection is a tuple, where:\n",
    "        - the first element is a list of parent subtitles, starting with the page title\n",
    "        - the second element is the text of the subsection (but not any children)\n",
    "    \"\"\"\n",
    "    headings = [str(h) for h in section.filter_headings()]\n",
    "    title = headings[0]\n",
    "    if title.strip(\"=\" + \" \") in sections_to_ignore:\n",
    "        # ^wiki headings are wrapped like \"== Heading ==\"\n",
    "        return []\n",
    "    titles = parent_titles + [title]\n",
    "    full_text = str(section)\n",
    "    section_text = full_text.split(title)[1]\n",
    "    if len(headings) == 1:\n",
    "        return [(titles, section_text)]\n",
    "    else:\n",
    "        first_subtitle = headings[1]\n",
    "        section_text = section_text.split(first_subtitle)[0]\n",
    "        results = [(titles, section_text)]\n",
    "        for subsection in section.get_sections(levels=[len(titles) + 1]):\n",
    "            results.extend(all_subsections_from_section(subsection, titles, sections_to_ignore))\n",
    "        return results\n",
    "\n",
    "\n",
    "def all_subsections_from_title(\n",
    "    title: str,\n",
    "    sections_to_ignore: set[str] = SECTIONS_TO_IGNORE,\n",
    "    site_name: str = WIKI_SITE,\n",
    ") -> list[tuple[list[str], str]]:\n",
    "    \"\"\"From a Wikipedia page title, return a flattened list of all nested subsections.\n",
    "    Each subsection is a tuple, where:\n",
    "        - the first element is a list of parent subtitles, starting with the page title\n",
    "        - the second element is the text of the subsection (but not any children)\n",
    "    \"\"\"\n",
    "    site = mwclient.Site(site_name)\n",
    "    page = site.pages[title]\n",
    "    text = page.text()\n",
    "    parsed_text = mwparserfromhell.parse(text)\n",
    "    headings = [str(h) for h in parsed_text.filter_headings()]\n",
    "    if headings:\n",
    "        summary_text = str(parsed_text).split(headings[0])[0]\n",
    "    else:\n",
    "        summary_text = str(parsed_text)\n",
    "    results = [([title], summary_text)]\n",
    "    for subsection in parsed_text.get_sections(levels=[2]):\n",
    "        results.extend(all_subsections_from_section(subsection, [title], sections_to_ignore))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 sections in 13 pages.\n"
     ]
    }
   ],
   "source": [
    "# split pages into sections\n",
    "# may take ~1 minute per 100 articles\n",
    "\n",
    "wikipedia_sections = []\n",
    "for title in titles:\n",
    "    wikipedia_sections.extend(all_subsections_from_title(title))\n",
    "print(f\"Found {len(wikipedia_sections)} sections in {len(titles)} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 0 sections, leaving 168 sections.\n"
     ]
    }
   ],
   "source": [
    "# clean text\n",
    "def clean_section(section: tuple[list[str], str]) -> tuple[list[str], str]:\n",
    "    \"\"\"\n",
    "    Return a cleaned up section with:\n",
    "        - <ref>xyz</ref> patterns removed\n",
    "        - leading/trailing whitespace removed\n",
    "    \"\"\"\n",
    "    titles, text = section\n",
    "    text = re.sub(r\"<ref.*?</ref>\", \"\", text)\n",
    "    text = text.strip()\n",
    "    return (titles, text)\n",
    "\n",
    "\n",
    "wikipedia_sections = [clean_section(ws) for ws in wikipedia_sections]\n",
    "\n",
    "# filter out short/blank sections\n",
    "def keep_section(section: tuple[list[str], str]) -> bool:\n",
    "    \"\"\"Return True if the section should be kept, False otherwise.\"\"\"\n",
    "    titles, text = section\n",
    "    if len(text) < 16:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "original_num_sections = len(wikipedia_sections)\n",
    "wikipedia_sections = [ws for ws in wikipedia_sections if keep_section(ws)]\n",
    "print(f\"Filtered out {original_num_sections - len(wikipedia_sections)} sections, leaving {len(wikipedia_sections)} sections.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2023 Cantabrian regional election']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{{current election}}\\n{{Infobox election\\n| election_name      = 2023 Cantabria...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['2023 Cantabrian regional election', '==Overview==', '===Electoral system===']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The [[Parliament of Cantabria]] is the [[Devolution|devolved]], [[unicameral ...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['2023 Cantabrian regional election', '==Overview==', '===Election date===']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The term of the Parliament of Cantabria expired four years after the date of ...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['2023 Cantabrian regional election', '==Parliamentary composition==']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The table below shows the composition of the parliamentary groups in the Parl...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['2023 Cantabrian regional election', '==Parties and candidates==']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The electoral law allows for [[Political party|parties]] and [[Political alli...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# print example data\n",
    "for ws in wikipedia_sections[:5]:\n",
    "    print(ws[0])\n",
    "    display(ws[1][:77] + \"...\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll recursively split long sections into smaller sections.\n",
    "There's no perfect recipe for splitting text into sections.\n",
    "Some tradeoffs include:\n",
    "- Longer sections may be better for questions that require more context\n",
    "- Longer sections may be worse for retrieval, as they may have more topics muddled together\n",
    "- Shorter sections are better for reducing costs (which are proportional to the number of tokens)\n",
    "- Shorter sections allow more sections to be retrieved, which may help with recall\n",
    "- Overlapping sections may help prevent answers from being cut by section boundaries\n",
    "- Here, we'll use a simple approach and limit sections to 1,600 tokens each, recursively halving any sections that are too long. To avoid cutting in the middle of useful sentences, we'll split along paragraph boundaries when possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_MODEL = \"gpt-3.5-turbo\"  # only matters insofar as it selects which tokenizer to use\n",
    "\n",
    "\n",
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n",
    "    \"\"\"Split a string in two, on a delimiter, trying to balance tokens on each side.\"\"\"\n",
    "    chunks = string.split(delimiter)\n",
    "    if len(chunks) == 1:\n",
    "        return [string, \"\"]  # no delimiter found\n",
    "    elif len(chunks) == 2:\n",
    "        return chunks  # no need to search for halfway point\n",
    "    else:\n",
    "        total_tokens = num_tokens(string)\n",
    "        halfway = total_tokens // 2\n",
    "        best_diff = halfway\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            left = delimiter.join(chunks[: i + 1])\n",
    "            left_tokens = num_tokens(left)\n",
    "            diff = abs(halfway - left_tokens)\n",
    "            if diff >= best_diff:\n",
    "                break\n",
    "            else:\n",
    "                best_diff = diff\n",
    "        left = delimiter.join(chunks[:i])\n",
    "        right = delimiter.join(chunks[i:])\n",
    "        return [left, right]\n",
    "\n",
    "\n",
    "def truncated_string(\n",
    "    string: str,\n",
    "    model: str,\n",
    "    max_tokens: int,\n",
    "    print_warning: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"Truncate a string to a maximum number of tokens.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    encoded_string = encoding.encode(string)\n",
    "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
    "    if print_warning and len(encoded_string) > max_tokens:\n",
    "        print(f\"Warning: Truncated string from {len(encoded_string)} tokens to {max_tokens} tokens.\")\n",
    "    return truncated_string\n",
    "\n",
    "\n",
    "def split_strings_from_subsection(\n",
    "    subsection: tuple[list[str], str],\n",
    "    max_tokens: int = 1000,\n",
    "    model: str = GPT_MODEL,\n",
    "    max_recursion: int = 5,\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Split a subsection into a list of subsections, each with no more than max_tokens.\n",
    "    Each subsection is a tuple of parent titles [H1, H2, ...] and text (str).\n",
    "    \"\"\"\n",
    "    titles, text = subsection\n",
    "    string = \"\\n\\n\".join(titles + [text])\n",
    "    num_tokens_in_string = num_tokens(string)\n",
    "    # if length is fine, return string\n",
    "    if num_tokens_in_string <= max_tokens:\n",
    "        return [string]\n",
    "    # if recursion hasn't found a split after X iterations, just truncate\n",
    "    elif max_recursion == 0:\n",
    "        return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
    "    # otherwise, split in half and recurse\n",
    "    else:\n",
    "        titles, text = subsection\n",
    "        for delimiter in [\"\\n\\n\", \"\\n\", \". \"]:\n",
    "            left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
    "            if left == \"\" or right == \"\":\n",
    "                # if either half is empty, retry with a more fine-grained delimiter\n",
    "                continue\n",
    "            else:\n",
    "                # recurse on each half\n",
    "                results = []\n",
    "                for half in [left, right]:\n",
    "                    half_subsection = (titles, half)\n",
    "                    half_strings = split_strings_from_subsection(\n",
    "                        half_subsection,\n",
    "                        max_tokens=max_tokens,\n",
    "                        model=model,\n",
    "                        max_recursion=max_recursion - 1,\n",
    "                    )\n",
    "                    results.extend(half_strings)\n",
    "                return results\n",
    "    # otherwise no split was found, so just truncate (should be very rare)\n",
    "    return [truncated_string(string, model=model, max_tokens=max_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168 Wikipedia sections split into 324 strings.\n"
     ]
    }
   ],
   "source": [
    "# split sections into chunks\n",
    "MAX_TOKENS = 1600\n",
    "wikipedia_strings = []\n",
    "for section in wikipedia_sections:\n",
    "    wikipedia_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
    "\n",
    "print(f\"{len(wikipedia_sections)} Wikipedia sections split into {len(wikipedia_strings)} strings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023 Cantabrian regional election\n",
      "\n",
      "==Parties and candidates==\n",
      "\n",
      "The electoral law allows for [[Political party|parties]] and [[Political alliance|federations]] registered in the [[Ministry of the Interior (Spain)|interior ministry]], [[Electoral alliance|coalitions]] and [[Grouping of electors (Spain)|groupings of electors]] to present lists of candidates. Parties and federations intending to form a coalition ahead of an election are required to inform the relevant Electoral Commission within ten days of the election call, whereas groupings of electors need to secure the signature of at least one percent of the electorate in Cantabria, disallowing electors from signing for more than one list of candidates.<ref name=\"CtbELaw\"/><ref name=\"SpaELaw\"/>\n",
      "\n",
      "Below is a list of the main parties and electoral alliances which will likely contest the election:\n",
      "\n",
      "{| class=\"wikitable\" style=\"line-height:1.35em; text-align:left;\"\n",
      "|-\n",
      "! colspan=\"2\" rowspan=\"2\"| Candidacy\n",
      "! rowspan=\"2\"| Parties and<br/>alliances\n",
      "! colspan=\"2\" rowspan=\"2\"| Candidate\n",
      "! rowspan=\"2\"| Ideology\n",
      "! colspan=\"2\"| Previous result\n",
      "! rowspan=\"2\"| {{abbr|Gov.|Government}}\n",
      "! rowspan=\"2\"| {{abbr|Ref.|References}}\n",
      "|-\n",
      "! Votes (%)\n",
      "! Seats\n",
      "|-\n",
      "| width=\"1\" bgcolor=\"{{party color|Regionalist Party of Cantabria}}\"|\n",
      "| align=\"center\"| '''[[Regionalist Party of Cantabria|PRC]]'''\n",
      "| {{Collapsible list\n",
      "| title = List\n",
      "| bullets = on\n",
      "| [[Regionalist Party of Cantabria]] (PRC)\n",
      "}}\n",
      "| [[File:Miguel Ángel Revilla 2019 (cropped).jpg|50px]]\n",
      "| [[Miguel Ángel Revilla]]\n",
      "| [[Regionalism (politics)|Regionalism]]<br/>[[Centrism]]\n",
      "| align=\"center\"| 37.64%\n",
      "| {{big|'''14'''}}\n",
      "| {{tick|15}}\n",
      "| \n",
      "|-\n",
      "| bgcolor=\"{{party color|People's Party of Cantabria}}\"|\n",
      "| align=\"center\"| '''[[People's Party of Cantabria|PP]]'''\n",
      "| {{Collapsible list\n",
      "| title = List\n",
      "| bullets = on\n",
      "| [[People's Party of Cantabria|People's Party]] (PP)\n",
      "}}\n",
      "| [[File:María José Sáenz de Buruaga 2018 (cropped).jpg|50px]]\n",
      "| [[María José Sáenz de Buruaga]]\n",
      "| [[Conservatism]]<br/>[[Christian democracy]]\n",
      "| align=\"center\"| 24.04%\n",
      "| {{big|'''9'''}}\n",
      "| {{xmark|15}}\n",
      "| \n",
      "|-\n",
      "| bgcolor=\"{{party color|Socialist Party of Cantabria}}\"|\n",
      "| align=\"center\"| '''[[Socialist Party of Cantabria|PSOE]]'''\n",
      "| {{Collapsible list\n",
      "| title = List\n",
      "| bullets = on\n",
      "| [[Socialist Party of Cantabria|Spanish Socialist Workers' Party]] (PSOE)\n",
      "}}\n",
      "| [[File:Pablo Zuloaga 2019 (cropped).jpg|50px]]\n",
      "| [[Pablo Zuloaga]]\n",
      "| [[Social democracy]]\n",
      "| align=\"center\"| 17.61%\n",
      "| {{big|'''7'''}}\n",
      "| {{tick|15}}\n",
      "| \n",
      "|-\n",
      "| bgcolor=\"{{party color|Citizens (Spanish political party)}}\"|\n",
      "| align=\"center\"| '''[[Citizens (Spanish political party)|CS]]'''\n",
      "| {{Collapsible list\n",
      "| title = List\n",
      "| bullets = on\n",
      "| [[Citizens (Spanish political party)|Citizens–Party of the Citizenry]] (CS)\n",
      "}}\n",
      "| [[File:Félix Álvarez Palleiro (cropped).jpg|50px]]\n",
      "| Félix Álvarez\n",
      "| [[Liberalism]]\n",
      "| align=\"center\"| 7.94%\n",
      "| {{big|'''3'''}}\n",
      "| {{xmark|15}}\n",
      "| \n",
      "|-\n",
      "| bgcolor=\"{{party color|Vox (political party)}}\"|\n",
      "| align=\"center\"| '''[[Vox (political party)|Vox]]'''\n",
      "| {{Collapsible list\n",
      "| title = List\n",
      "| bullets = on\n",
      "| [[Vox (political party)|Vox]] (Vox)\n",
      "}}\n",
      "| [[File:Leticia Díaz 2023 (cropped).jpg|50px]]\n",
      "| Leticia Díaz\n",
      "| [[Right-wing populism]]<br/>[[Ultranationalism]]<br/>[[National conservatism]]\n",
      "| align=\"center\"| 5.06%\n",
      "| {{big|'''2'''}}\n",
      "| {{xmark|15}}\n",
      "| \n",
      "|-\n",
      "| bgcolor=\"{{party color|Unidas Podemos}}\"|\n",
      "| align=\"center\"| '''[[Unidas Podemos|Podemos–IU]]'''\n",
      "| {{Collapsible list\n",
      "| title = List\n",
      "| bullets = on\n",
      "| [[Podemos (Spanish political party)|We Can]] (Podemos)\n",
      "| [[United Left of Cantabria|United Left]] (IU)<br/>{{smaller|– [[Communist Party of Cantabria]] (PCC)<br/>– [[Revolutionary Workers' Party (Spain)|The Dawn. Marxist Organization OM]] (La Aurora (om))<br/>– [[Republican Left (Spain, 1977)|Republican Left]] (IR)}}\n",
      "}}\n",
      "| [[File:Mónica Rodero 2023 (cropped).jpg|50px]]\n",
      "| Mónica Rodero\n",
      "| [[Left-wing populism]]<br/>[[Direct democracy]]<br/>[[Democratic socialism]]\n",
      "| align=\"center\"| 5.04%{{efn|name=\"Podemos+IU\"|Results for [[Podemos (Spanish political party)|Podemos]] (3.14%, 0 seats) and [[United Left of Cantabria|IU]]+[[Equo|Equo (Marea Cántabra)]] (1.90%, 0 seats) in the 2019 election.}}\n",
      "| {{big|'''0'''}}\n",
      "| {{xmark|15}}\n",
      "| <br/>\n",
      "|}\n",
      "\n",
      "On 19 December 2019, Félix Álvarez resigned as leader of [[Citizens (Spanish political party)|Citizens]] (CS) in Cantabria, citing \"disagreements\" with the party's leadership after a scandal broke out over the one-day hiring of Cs former leading candidate for the [[Congress of Deputies]] in the region, Rubén Gómez, a contract which Álvarez had publicly denied from having taken place.\n"
     ]
    }
   ],
   "source": [
    "# print example data\n",
    "print(wikipedia_strings[4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Embed document chunks\n",
    "Now that we've split our library into shorter self-contained strings, we can compute embeddings for each.\n",
    "\n",
    "!!! For large embedding jobs, use a script like api_request_parallel_processor.py to parallelize requests while throttling to stay under rate limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 to 99\n",
      "Batch 100 to 199\n",
      "Batch 200 to 299\n",
      "Batch 300 to 399\n"
     ]
    }
   ],
   "source": [
    "# calculate embeddings\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"  # OpenAI's best embeddings as of Apr 2023\n",
    "BATCH_SIZE = 100  # you can submit up to 2048 embedding inputs per request\n",
    "\n",
    "embeddings = []\n",
    "for batch_start in range(0, len(wikipedia_strings), BATCH_SIZE):\n",
    "    batch_end = batch_start + BATCH_SIZE\n",
    "    batch = wikipedia_strings[batch_start:batch_end]\n",
    "    print(f\"Batch {batch_start} to {batch_end-1}\")\n",
    "    response = openai.Embedding.create(model=EMBEDDING_MODEL, input=batch)\n",
    "    for i, be in enumerate(response[\"data\"]):\n",
    "        assert i == be[\"index\"]  # double check embeddings are in same order as input\n",
    "    batch_embeddings = [e[\"embedding\"] for e in response[\"data\"]]\n",
    "    embeddings.extend(batch_embeddings)\n",
    "\n",
    "df = pd.DataFrame({\"text\": wikipedia_strings, \"embedding\": embeddings})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Store document chunks and embeddings\n",
    "Because this example only uses a few thousand strings, we'll store them in a CSV file.\n",
    "For larger datasets, use a vector database, which will be more performant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save document chunks and embeddings\n",
    "\n",
    "SAVE_PATH = \"C:/Users/luisg/Downloads/elecciones.csv\"\n",
    "\n",
    "df.to_csv(SAVE_PATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
